<!doctype html>
<html>
<head>
<meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="left_from_right.css">
<title>Telling Left from Right: Learning Spatial Correspondence of Sight and Sound</title>
</head>

<body>
<div align="center">
  <h1>Telling Left from Right: Learning Spatial Correspondence of Sight and Sound</h1>
  <span style="font-size: 20px"> CVPR 2020 (Oral Presentation)</span><br>
    <br>
    <span style="font-size: 20px"> 
		Karren Yang&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <a href="https://bryanrussell.org">Bryan Russell</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <a href="https://justinsalamon.com">Justin Salamon</a></span>
	<br>
	<span style="font-size:20px"> 
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    MIT &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	Adobe Research &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	Adobe Research 
	</span>
	<br>
</div>
<br>
<br>
<div align="center">
	<span style="font-size: 25px">
		&nbsp;&nbsp;&nbsp; Paper &nbsp;&nbsp;&nbsp; | 
		&nbsp;&nbsp;&nbsp; 
		<a href="https://drive.google.com/file/d/1uTPYWa2a8nF35Y6oqT8GFiqqxCYBnPNK/view?usp=sharing">YouTube-ASMR Dataset</a>
		&nbsp;&nbsp;&nbsp; |
		&nbsp;&nbsp;&nbsp; 
		<a href="https://drive.google.com/file/d/1ED6bwwohbJd9xeuX7KUZC2I9Jqs8nr-K/view?usp=sharing">
		Slides
		</a>
		&nbsp;&nbsp;&nbsp;
	</span>
	
</div>
	
<div align="left">
	<h2> Abstract </h2>
	
	Self-supervised audio-visual learning aims to capture useful representations of video by leveraging correspondences between visual and audio inputs. Existing approaches have focused primarily on matching semantic information btween the sensory streams. We propose a novel self-superivsed task to leverage an orthogonal principle: matching spatial information in the audio stream to the positions of sound sources in the visual stream. Our approach is simple yet effective. We train a model to determine whether the left and right aduio channels have been flipped, forcing it to reason about spatial localization across the visual and audio streams. To train and evaluate our model, we introduce a large-scale video datasets, YouTube-ASMR-300K, with spatial audio comprising over 900 hours of footage. We demonstrate that understanding spatial correspondence enables models to peform better on three audio-visual tasks, achieving quantitative gains over supervised and self-supervised baselines that do not leverage spatial audio cues. We also show how to extend our self-supervised approach to 360 degree videos with ambisonic audio.
</div>
<div>
	<span align="left">
		<h2> Video </h2>
	</span>
  <span align="center">
	<video controls width="650" style="display:block; margin:0 auto;">
	    <source src="media/talk.mp4" type="video/mp4">
	</video>
	</span>
	
</div>
</body>
</html>
